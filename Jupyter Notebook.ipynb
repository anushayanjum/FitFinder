{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Finder - A Resume Classification Project\n",
    "\n",
    "### Anushay Anjum and Ayanna Negi\n",
    "\n",
    "### Special Topics in Data Science \n",
    "\n",
    "### Professor Sid Sah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "This notebook demonstrates the complete process of analyzing a dataset of resumes. We:\n",
    "- Load and describe the data (from Kaggle: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset)\n",
    "- Clean and preprocess textual data\n",
    "- Extract skills and experience years\n",
    "- Perform a statistical test (t-test) comparing two categories\n",
    "- Compute and interpret correlation\n",
    "- Discuss why correlation doesn't imply causation\n",
    "- Build a predictive model (logistic regression) with SMOTE and threshold tuning\n",
    "- Provide visualizations (boxplots, ROC curves)\n",
    "\n",
    "### Requirements:\n",
    "- resume.csv in the same directory\n",
    "- resume.pdf (user's uploaded resume) in the same directory\n",
    "- Install necessary libraries:\n",
    "\n",
    "    `pip install spacy pdfplumber sentence-transformers imbalanced-learn seaborn matplotlib scikit-learn`\n",
    "\n",
    "Rubric Components:\n",
    "- Data description\n",
    "- Statistical test\n",
    "- Variable relationships & correlation\n",
    "- Causality discussion\n",
    "- Predictive model (logistic regression)\n",
    "- Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    roc_curve, \n",
    "    roc_auc_score, \n",
    "    f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'resume.csv'            \n",
    "UPLOADED_PDF_PATH = 'resume.pdf'   # The user's uploaded resume in PDF form\n",
    "SENTENCE_BERT_MODEL = 'all-MiniLM-L6-v2'  # Sentence-BERT model\n",
    "CHOSEN_INDUSTRY = \"Information-Technology\"  \n",
    "CHOSEN_EXPERIENCE = \"junior\"\n",
    "SKILL_MULTIPLIER = 2.0  \n",
    "\n",
    "CATEGORY_SKILLS = {\n",
    "    'HR': [\n",
    "        'recruiting', 'employee relations', 'benefits', 'compliance', \n",
    "        'training', 'talent acquisition', 'performance management'\n",
    "    ],\n",
    "    'Designer': [\n",
    "        'graphic design', 'ui', 'ux', 'illustration', 'branding', \n",
    "        'adobe', 'prototyping', 'typography'\n",
    "    ],\n",
    "    'Information-Technology': [\n",
    "        'network', 'java', 'python', 'sql', \n",
    "        'cloud', 'help desk', 'project management', 'devops'\n",
    "    ],\n",
    "    'Teacher': [\n",
    "        'curriculum', 'classroom management', 'lesson planning', \n",
    "        'assessment', 'differentiated instruction', 'edtech', 'pedagogy'\n",
    "    ],\n",
    "    'Advocate': [\n",
    "        'legal research', 'client advocacy', 'litigation', \n",
    "        'policy', 'negotiation', 'case management', 'contract drafting'\n",
    "    ],\n",
    "    'Business-Development': [\n",
    "        'lead generation', 'partnership', 'market research', \n",
    "        'sales strategy', 'business strategy', 'negotiation', 'strategic planning'\n",
    "    ],\n",
    "    'Healthcare': [\n",
    "        'patient care', 'medical terminology', 'hipaa', \n",
    "        'clinical', 'diagnostics', 'health education', 'care coordination'\n",
    "    ],\n",
    "    'Fitness': [\n",
    "        'personal training', 'exercise', 'nutrition', \n",
    "        'wellness', 'fitness assessments', 'group fitness', 'strength training'\n",
    "    ],\n",
    "    'Agriculture': [\n",
    "        'crop management', 'soil', 'irrigation', \n",
    "        'farm machinery', 'sustainable farming', 'pest control', 'harvest'\n",
    "    ],\n",
    "    'BPO': [\n",
    "        'customer service', 'call center', 'data entry', \n",
    "        'outsourcing', 'telemarketing', 'crm', 'sla management'\n",
    "    ],\n",
    "    'Sales': [\n",
    "        'prospecting', 'lead nurturing', 'cold calling', 'closing deals', \n",
    "        'account management', 'salesforce', 'pipeline management'\n",
    "    ],\n",
    "    'Consultant': [\n",
    "        'strategic planning', 'market analysis', 'process improvement', \n",
    "        'stakeholder engagement', 'data analysis', 'client presentations', 'change management'\n",
    "    ],\n",
    "    'Digital-Media': [\n",
    "        'social media', 'content creation', 'seo', 'sem', \n",
    "        'video editing', 'analytics', 'influencer', 'campaign management'\n",
    "    ],\n",
    "    'Automobile': [\n",
    "        'vehicle maintenance', 'automotive engineering', 'diagnostic tools', \n",
    "        'car sales', 'supply chain', 'quality control', 'auto repair'\n",
    "    ],\n",
    "    'Chef': [\n",
    "        'menu planning', 'food preparation', 'culinary', \n",
    "        'kitchen management', 'food safety', 'nutrition', 'inventory control'\n",
    "    ],\n",
    "    'Finance': [\n",
    "        'financial analysis', 'budgeting', 'accounting', 'forecasting', \n",
    "        'investments', 'risk management', 'excel', 'financial modeling'\n",
    "    ],\n",
    "    'Apparel': [\n",
    "        'fashion design', 'textile', 'merchandising', \n",
    "        'pattern making', 'quality control', 'trend analysis', 'inventory management'\n",
    "    ],\n",
    "    'Engineering': [\n",
    "        'cad', 'mechanical design', 'electrical engineering', 'system design', \n",
    "        'project management', 'quality assurance', 'prototyping'\n",
    "    ],\n",
    "    'Accountant': [\n",
    "        'bookkeeping', 'tax preparation', 'financial reporting', \n",
    "        'auditing', 'sap', 'quickbooks', 'gaap', 'reconciliation'\n",
    "    ],\n",
    "    'Construction': [\n",
    "        'blueprint', 'project scheduling', 'safety compliance', \n",
    "        'cost estimation', 'material procurement', 'team management', 'budgeting'\n",
    "    ],\n",
    "    'Public-Relations': [\n",
    "        'press releases', 'media relations', 'crisis management', \n",
    "        'branding', 'event planning', 'social media', 'stakeholder communication'\n",
    "    ],\n",
    "    'Banking': [\n",
    "        'loan processing', 'credit analysis', 'customer service', \n",
    "        'risk assessment', 'investment services', 'compliance', 'financial advisement'\n",
    "    ],\n",
    "    'Arts': [\n",
    "        'artistic', 'art history', 'painting', \n",
    "        'sculpting', 'graphic design', 'photography', 'visual storytelling'\n",
    "    ],\n",
    "    'Aviation': [\n",
    "        'flight operations', 'aircraft maintenance', 'air traffic control', \n",
    "        'aerodynamics', 'safety regulations', 'logistics', 'flight planning'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Configuration + Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:29:46,163 [INFO] Loaded spaCy 'en_core_web_sm' model successfully.\n"
     ]
    }
   ],
   "source": [
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"project_log.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    logger.info(\"Loaded spaCy 'en_core_web_sm' model successfully.\")\n",
    "except OSError:\n",
    "    logger.info(\"spaCy model 'en_core_web_sm' not found. Downloading...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    logger.info(\"Downloaded and loaded spaCy 'en_core_web_sm' model.\")\n",
    "\n",
    "def check_file_exists(file_path: str) -> bool:\n",
    "    exists = os.path.exists(file_path)\n",
    "    if not exists:\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "    return exists\n",
    "\n",
    "def clean_and_lemmatize(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    if not check_file_exists(pdf_path):\n",
    "        return \"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        logger.info(f\"Extracted text from PDF: {pdf_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def create_industry_experience_prompt(industry: str, experience_level: str) -> str:\n",
    "    return f\"Looking for a candidate in the {industry} industry with {experience_level} level experience.\"\n",
    "\n",
    "def count_skills_in_text(text: str, skills: list) -> int:\n",
    "    text_lower = text.lower() if isinstance(text, str) else \"\"\n",
    "    count = 0\n",
    "    for skill in skills:\n",
    "        count += text_lower.count(skill.lower())\n",
    "    return count\n",
    "\n",
    "def extract_years_of_experience(text: str) -> int:\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    matches = re.findall(r'(\\d+)\\s+years', text.lower())\n",
    "    if matches:\n",
    "        return max(int(m) for m in matches)\n",
    "    return 0\n",
    "\n",
    "def categorize_experience_years(years: int) -> str:\n",
    "    if years < 3:\n",
    "        return 'junior'\n",
    "    elif years < 8:\n",
    "        return 'mid'\n",
    "    elif years < 15:\n",
    "        return 'senior'\n",
    "    else:\n",
    "        return 'executive'\n",
    "\n",
    "def experience_to_numeric(level: str) -> int:\n",
    "    mapping = {'junior': 1, 'mid': 2, 'senior': 3, 'executive': 4}\n",
    "    return mapping.get(level, 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not check_file_exists(CSV_PATH):\n",
    "        logger.error(\"Exiting program due to missing CSV file.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(CSV_PATH)\n",
    "        logger.info(\"Loaded data from CSV successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading CSV file {CSV_PATH}: {e}\")\n",
    "        return\n",
    "\n",
    "    required_columns = ['ID', 'Resume_str', 'Resume_html', 'Category']\n",
    "    for col in required_columns:\n",
    "        if col not in data.columns:\n",
    "            logger.error(f\"CSV file must contain a '{col}' column.\")\n",
    "            return\n",
    "\n",
    "    # Data Cleaning\n",
    "    logger.info(\"Preprocessing resumes from dataset...\")\n",
    "    data['Cleaned_Resume'] = data['Resume_str'].apply(clean_and_lemmatize)\n",
    "\n",
    "    # Load Sentence-BERT model\n",
    "    try:\n",
    "        logger.info(\"Loading Sentence-BERT model...\")\n",
    "        model = SentenceTransformer(SENTENCE_BERT_MODEL)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading Sentence-BERT model '{SENTENCE_BERT_MODEL}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Encode dataset resumes\n",
    "    try:\n",
    "        logger.info(\"Encoding dataset resumes with Sentence-BERT...\")\n",
    "        dataset_embeddings = model.encode(\n",
    "            data['Cleaned_Resume'].tolist(),\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=32,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during dataset resume embedding: {e}\")\n",
    "        return\n",
    "\n",
    "    # User Resume\n",
    "    user_resume_text = extract_text_from_pdf(UPLOADED_PDF_PATH)\n",
    "    user_cleaned_resume = clean_and_lemmatize(user_resume_text)\n",
    "    user_embedding = model.encode([user_cleaned_resume], convert_to_tensor=True)\n",
    "\n",
    "    query_text = create_industry_experience_prompt(CHOSEN_INDUSTRY, CHOSEN_EXPERIENCE)\n",
    "    query_embedding = model.encode([query_text], convert_to_tensor=True)\n",
    "\n",
    "    user_score = util.cos_sim(user_embedding, query_embedding).item()\n",
    "    logger.info(f\"User Resume Suitability Score for {CHOSEN_INDUSTRY} ({CHOSEN_EXPERIENCE} level): {user_score:.4f}\")\n",
    "\n",
    "    all_scores = util.cos_sim(user_embedding, dataset_embeddings).cpu().numpy().flatten()\n",
    "    data['User_Similarity'] = all_scores\n",
    "\n",
    "    # Skill-Based Features\n",
    "    chosen_skills = CATEGORY_SKILLS.get(CHOSEN_INDUSTRY, [])\n",
    "    data['Skill_Count'] = data['Resume_str'].apply(lambda x: count_skills_in_text(x, chosen_skills))\n",
    "\n",
    "    # Experience Extraction\n",
    "    data['Years_Exp'] = data['Resume_str'].apply(extract_years_of_experience)\n",
    "    data['Experience_Level'] = data['Years_Exp'].apply(categorize_experience_years)\n",
    "    data['Experience_Num'] = data['Experience_Level'].apply(experience_to_numeric)\n",
    "\n",
    "    user_years_exp = extract_years_of_experience(user_resume_text)\n",
    "    user_exp_level = categorize_experience_years(user_years_exp)\n",
    "    user_exp_num = experience_to_numeric(user_exp_level)\n",
    "    logger.info(f\"User Resume Experience Level: {user_exp_level} ({user_years_exp} years)\")\n",
    "\n",
    "    # Statistical Test (Engineering vs Finance)\n",
    "    eng_scores = data.loc[data['Category'].str.lower() == 'engineering', 'User_Similarity']\n",
    "    fin_scores = data.loc[data['Category'].str.lower() == 'finance', 'User_Similarity']\n",
    "    if len(eng_scores) > 1 and len(fin_scores) > 1:\n",
    "        t_stat, p_val = ttest_ind(eng_scores, fin_scores, equal_var=False, nan_policy='omit')\n",
    "        logger.info(f\"T-statistic: {t_stat:.4f}, P-value: {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            logger.info(\"We reject the null hypothesis and conclude there's a significant difference.\")\n",
    "        else:\n",
    "            logger.info(\"We fail to reject the null hypothesis; no significant difference found.\")\n",
    "    else:\n",
    "        logger.info(\"Not enough data to perform the statistical test.\")\n",
    "\n",
    "    # Correlation\n",
    "    category_to_num = {cat: i for i, cat in enumerate(sorted(data['Category'].unique()))}\n",
    "    data['Category_Num'] = data['Category'].map(category_to_num)\n",
    "    corr = data['User_Similarity'].corr(data['Category_Num'])\n",
    "    logger.info(f\"Correlation between category numeric encoding and User_Similarity: {corr:.4f}\")\n",
    "\n",
    "    # Predictive Model\n",
    "    data['Good_Fit'] = data['Category'].apply(lambda c: 1 if c.lower() == CHOSEN_INDUSTRY.lower() else 0)\n",
    "\n",
    "    # Weighted skill features\n",
    "    data['Weighted_Skill'] = data['Skill_Count'] * SKILL_MULTIPLIER\n",
    "\n",
    "    # Features: User_Similarity, Weighted_Skill, Experience_Num\n",
    "    X = data[['User_Similarity', 'Weighted_Skill', 'Experience_Num']].fillna(0)\n",
    "    y = data['Good_Fit']\n",
    "\n",
    "    # Scale features for better performance\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Handle imbalance with SMOTE\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Logistic Regression with class_weight\n",
    "    model_lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "    model_lr.fit(X_train_res, y_train_res)\n",
    "\n",
    "    y_pred_proba = model_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Threshold Tuning\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0\n",
    "    for t in thresholds:\n",
    "        y_pred_adj = (y_pred_proba > t).astype(int)\n",
    "        f1_pos = f1_score(y_test, y_pred_adj, pos_label=1)\n",
    "        if f1_pos > best_f1:\n",
    "            best_f1 = f1_pos\n",
    "            best_thresh = t\n",
    "\n",
    "    logger.info(f\"Best threshold found: {best_thresh:.2f} with F1-score for positive class: {best_f1:.4f}\")\n",
    "    y_pred = (y_pred_proba > best_thresh).astype(int)\n",
    "\n",
    "    logger.info(\"Classification Report after Threshold Tuning:\")\n",
    "    logger.info(\"\\n\" + classification_report(y_test, y_pred))\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    logger.info(f\"Model AUC: {auc:.4f}\")\n",
    "\n",
    "    # Data Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Category', y='User_Similarity', data=data)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('User Similarity Scores by Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"category_similarity_boxplot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Good_Fit Prediction')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"roc_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    logger.info(\"Data visualization saved as 'category_similarity_boxplot.png' and 'roc_curve.png'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 16:29:46,919 [INFO] Loaded data from CSV successfully.\n",
      "2024-12-11 16:29:46,926 [INFO] Preprocessing resumes from dataset...\n",
      "2024-12-11 16:36:14,610 [INFO] Loading Sentence-BERT model...\n",
      "2024-12-11 16:36:14,635 [INFO] Use pytorch device_name: cpu\n",
      "2024-12-11 16:36:14,637 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2024-12-11 16:36:16,595 [INFO] Encoding dataset resumes with Sentence-BERT...\n",
      "Batches: 100%|██████████| 78/78 [02:22<00:00,  1.82s/it]\n",
      "2024-12-11 16:38:39,921 [INFO] Extracted text from PDF: resume.pdf\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.44it/s]\n",
      "2024-12-11 16:38:40,294 [INFO] User Resume Suitability Score for Information-Technology (junior level): 0.3122\n",
      "2024-12-11 16:38:41,214 [INFO] User Resume Experience Level: junior (0 years)\n",
      "2024-12-11 16:38:41,313 [INFO] T-statistic: 8.3631, P-value: 0.0000\n",
      "2024-12-11 16:38:41,314 [INFO] We reject the null hypothesis and conclude there's a significant difference.\n",
      "2024-12-11 16:38:41,337 [INFO] Correlation between category numeric encoding and User_Similarity: 0.1011\n",
      "2024-12-11 16:38:41,646 [INFO] Best threshold found: 0.94 with F1-score for positive class: 0.5882\n",
      "2024-12-11 16:38:41,647 [INFO] Classification Report after Threshold Tuning:\n",
      "2024-12-11 16:38:41,655 [INFO] \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       473\n",
      "           1       0.56      0.62      0.59        24\n",
      "\n",
      "    accuracy                           0.96       497\n",
      "   macro avg       0.77      0.80      0.78       497\n",
      "weighted avg       0.96      0.96      0.96       497\n",
      "\n",
      "2024-12-11 16:38:41,665 [INFO] Model AUC: 0.9066\n",
      "2024-12-11 16:38:42,390 [INFO] Data visualization saved as 'category_similarity_boxplot.png' and 'roc_curve.png'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
